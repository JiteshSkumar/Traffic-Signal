{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q efficientnet\n!pip install keras-rectified-adam\n!pip install keras-lookahead\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nimport efficientnet.tfkeras as efn\nimport dill\nfrom tensorflow.keras import backend as K\nimport tensorflow_addons as tfa\nfrom keras_radam import RAdam\nfrom keras_lookahead import Lookahead\nfrom tensorflow.keras.callbacks import ModelCheckpoint","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-01T06:53:41.516066Z","iopub.execute_input":"2023-11-01T06:53:41.516666Z","iopub.status.idle":"2023-11-01T06:54:42.089926Z","shell.execute_reply.started":"2023-11-01T06:53:41.516611Z","shell.execute_reply":"2023-11-01T06:54:42.088917Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting keras-rectified-adam\n  Downloading keras-rectified-adam-0.20.0.tar.gz (7.0 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from keras-rectified-adam) (1.23.5)\nRequirement already satisfied: Keras in /usr/local/lib/python3.8/site-packages (from keras-rectified-adam) (2.12.0)\nBuilding wheels for collected packages: keras-rectified-adam\n  Building wheel for keras-rectified-adam (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-rectified-adam: filename=keras_rectified_adam-0.20.0-py3-none-any.whl size=8271 sha256=36f0ded84fddc6a354cf1fe4ffd65d1465164da3cce2de23e678d995bc135ade\n  Stored in directory: /root/.cache/pip/wheels/5d/0c/92/c76f52204fb345e4495a09929ffa813a0d790e971f6064a6c9\nSuccessfully built keras-rectified-adam\nInstalling collected packages: keras-rectified-adam\nSuccessfully installed keras-rectified-adam-0.20.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting keras-lookahead\n  Downloading keras-lookahead-0.9.0.tar.gz (5.3 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from keras-lookahead) (1.23.5)\nRequirement already satisfied: Keras in /usr/local/lib/python3.8/site-packages (from keras-lookahead) (2.12.0)\nBuilding wheels for collected packages: keras-lookahead\n  Building wheel for keras-lookahead (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-lookahead: filename=keras_lookahead-0.9.0-py3-none-any.whl size=6402 sha256=067961d3c7cb3891e7af9f48bf4c3cfca85ab365a841a27299e432b787db338d\n  Stored in directory: /root/.cache/pip/wheels/51/6d/2c/fc2fe9362e1159ba178f3bcc96feb5d9d392ed28d20db25728\nSuccessfully built keras-lookahead\nInstalling collected packages: keras-lookahead\nSuccessfully installed keras-lookahead-0.9.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"D1101 06:54:34.929979200      15 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD1101 06:54:34.930012491      15 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD1101 06:54:34.930016641      15 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD1101 06:54:34.930019661      15 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD1101 06:54:34.930022661      15 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD1101 06:54:34.930025639      15 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD1101 06:54:34.930033074      15 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD1101 06:54:34.930035959      15 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD1101 06:54:34.930038465      15 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD1101 06:54:34.930040963      15 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD1101 06:54:34.930043532      15 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD1101 06:54:34.930046318      15 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD1101 06:54:34.930049309      15 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD1101 06:54:34.930051945      15 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI1101 06:54:34.930371258      15 ev_epoll1_linux.cc:122]               grpc epoll fd: 63\nD1101 06:54:34.949839828      15 ev_posix.cc:144]                      Using polling engine: epoll1\nD1101 06:54:34.949890673      15 dns_resolver_ares.cc:822]             Using ares dns resolver\nD1101 06:54:34.950404501      15 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD1101 06:54:34.950422500      15 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD1101 06:54:34.950427125      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD1101 06:54:34.950430677      15 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD1101 06:54:34.950434337      15 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD1101 06:54:34.950437786      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD1101 06:54:34.950446502      15 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD1101 06:54:34.950479348      15 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD1101 06:54:34.950516452      15 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD1101 06:54:34.950534938      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD1101 06:54:34.950539015      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD1101 06:54:34.950542428      15 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD1101 06:54:34.950551089      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD1101 06:54:34.950554773      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD1101 06:54:34.950558500      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD1101 06:54:34.950561877      15 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI1101 06:54:34.954902093      15 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI1101 06:54:34.971140382     397 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE1101 06:54:34.976848632     397 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2023-11-01T06:54:34.976830816+00:00\", grpc_status:2}\n/usr/local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:54:42.091578Z","iopub.execute_input":"2023-11-01T06:54:42.092079Z","iopub.status.idle":"2023-11-01T06:54:49.967131Z","shell.execute_reply.started":"2023-11-01T06:54:42.092052Z","shell.execute_reply":"2023-11-01T06:54:49.965182Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Running on TPU  \nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"REPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\nGCS_PATH = KaggleDatasets().get_gcs_path('melanoma-256x256')\n\n# Configuration\nEPOCHS = 25    \nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nAUG_BATCH = BATCH_SIZE\nIMAGE_SIZE = [256, 256]\n# Seed\nSEED = 123\n# Learning rate\nLR = 0.00001\n# cutmix prob\ncutmix_rate = 0.30\n\n# training filenames directory\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')\n# test filenames directory\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test*.tfrec')\n\n\n# submission file\nSUB = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:54:49.968052Z","iopub.execute_input":"2023-11-01T06:54:49.968273Z","iopub.status.idle":"2023-11-01T06:54:50.029117Z","shell.execute_reply.started":"2023-11-01T06:54:49.968252Z","shell.execute_reply":"2023-11-01T06:54:50.028192Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"get_gcs_path is not required on TPU VMs which can directly use Kaggle datasets, using path: /kaggle/input/melanoma-256x256\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:54:50.030676Z","iopub.execute_input":"2023-11-01T06:54:50.030972Z","iopub.status.idle":"2023-11-01T06:54:50.039921Z","shell.execute_reply.started":"2023-11-01T06:54:50.030947Z","shell.execute_reply":"2023-11-01T06:54:50.039062Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def transform(image, label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 #fix for size 331\n    \n    tmp = random.uniform(0, 1)\n    if 0 < tmp <= 0.1:\n        rot = 15.0 * tf.random.normal([1],dtype='float32')\n    elif 0.1 < tmp <= 0.2:\n        rot = 30.0 * tf.random.normal([1],dtype='float32')\n    elif 0.2 < tmp <= 0.3:\n        rot = 45.0 * tf.random.normal([1],dtype='float32')\n    elif 0.3 < tmp <= 0.4:\n        rot = 60.0 * tf.random.normal([1],dtype='float32')\n    elif 0.4 < tmp <= 0.5:\n        rot = 75.0 * tf.random.normal([1],dtype='float32')\n    elif 0.5 < tmp <= 0.6:\n        rot = 90.0 * tf.random.normal([1],dtype='float32')\n    elif 0.6 < tmp <= 0.7:\n        rot = 110.0 * tf.random.normal([1],dtype='float32')\n    elif 0.7 < tmp <= 0.8:\n        rot = 130.0 * tf.random.normal([1],dtype='float32')\n    elif 0.8 < tmp <= 0.9:\n        rot = 150.0 * tf.random.normal([1],dtype='float32')\n    elif 0.9 < tmp <= 1.0:\n        rot = 180.0 * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n    d = tf.gather_nd(image['inp1'],tf.transpose(idx3))\n        \n    return {'inp1': tf.reshape(d,[DIM,DIM,3]), 'inp2': image['inp2']}, label\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:55:46.678607Z","iopub.execute_input":"2023-11-01T06:55:46.679292Z","iopub.status.idle":"2023-11-01T06:55:46.692425Z","shell.execute_reply.started":"2023-11-01T06:55:46.679259Z","shell.execute_reply":"2023-11-01T06:55:46.691658Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# function to apply cutmix augmentation\ndef cutmix(image, label):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with cutmix applied\n    \n    DIM = IMAGE_SIZE[0]    \n    imgs = []; labs = []\n    \n    for j in range(BATCH_SIZE):\n        \n        #random_uniform( shape, minval=0, maxval=None)        \n        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n        P = tf.cast(tf.random.uniform([], 0, 1) <= cutmix_rate, tf.int32)\n        \n        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n        k = tf.cast(tf.random.uniform([], 0, BATCH_SIZE), tf.int32)\n        \n        # CHOOSE RANDOM LOCATION\n        x = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\n        y = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\n        \n        # Beta(1, 1)\n        b = tf.random.uniform([], 0, 1) # this is beta dist with alpha=1.0\n        \n\n        WIDTH = tf.cast(DIM * tf.math.sqrt(1-b),tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH//2)\n        yb = tf.math.minimum(DIM,y+WIDTH//2)\n        xa = tf.math.maximum(0,x-WIDTH//2)\n        xb = tf.math.minimum(DIM,x+WIDTH//2)\n        \n        # MAKE CUTMIX IMAGE\n        one = image['inp1'][j,ya:yb,0:xa,:]\n        two = image['inp1'][k,ya:yb,xa:xb,:]\n        three = image['inp1'][j,ya:yb,xb:DIM,:]        \n        #ya:yb\n        middle = tf.concat([one,two,three],axis=1)\n\n        img = tf.concat([image['inp1'][j,0:ya,:,:],middle,image['inp1'][j,yb:DIM,:,:]],axis=0)\n        imgs.append(img)\n        \n        # MAKE CUTMIX LABEL\n        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n        lab1 = label[j,]\n        lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n\n    image2 = tf.reshape(tf.stack(imgs),(BATCH_SIZE,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(BATCH_SIZE, 1))\n    return {'inp1': image2, 'inp2': image['inp2']}, label2\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:55:51.342392Z","iopub.execute_input":"2023-11-01T06:55:51.343245Z","iopub.status.idle":"2023-11-01T06:55:51.354181Z","shell.execute_reply.started":"2023-11-01T06:55:51.343212Z","shell.execute_reply":"2023-11-01T06:55:51.353384Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:55:54.796472Z","iopub.execute_input":"2023-11-01T06:55:54.796809Z","iopub.status.idle":"2023-11-01T06:55:54.801478Z","shell.execute_reply.started":"2023-11-01T06:55:54.796781Z","shell.execute_reply":"2023-11-01T06:55:54.800689Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# function to decode our images (normalize and reshape)\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    # convert image to floats in [0, 1] range\n    image = tf.cast(image, tf.float32) / 255.0 \n    # explicit size needed for TPU\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:56:11.589556Z","iopub.execute_input":"2023-11-01T06:56:11.589931Z","iopub.status.idle":"2023-11-01T06:56:11.594717Z","shell.execute_reply.started":"2023-11-01T06:56:11.589901Z","shell.execute_reply":"2023-11-01T06:56:11.593957Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# this function parse our images and also get the target variable\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        # tf.string means bytestring\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        # shape [] means single element\n        \"target\": tf.io.FixedLenFeature([], tf.int64),\n        # meta features\n        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([],   tf.int64)\n        \n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['target'], tf.float32)\n    # meta features\n    data = {}\n    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n    data['sex'] = tf.cast(example['sex'], tf.int32)\n    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n    # returns a dataset of (image, label, data)\n    return image, label, data\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:56:25.734697Z","iopub.execute_input":"2023-11-01T06:56:25.735069Z","iopub.status.idle":"2023-11-01T06:56:25.742170Z","shell.execute_reply.started":"2023-11-01T06:56:25.735040Z","shell.execute_reply":"2023-11-01T06:56:25.741305Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# this function parse our image and also get our image_name (id) to perform predictions\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        # tf.string means bytestring\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        # shape [] means single element\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        # meta features\n        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    image_name = example['image_name']\n    # meta features\n    data = {}\n    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n    data['sex'] = tf.cast(example['sex'], tf.int32)\n    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n    # returns a dataset of (image, key, data)\n    return image, image_name, data\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:56:39.062109Z","iopub.execute_input":"2023-11-01T06:56:39.062466Z","iopub.status.idle":"2023-11-01T06:56:39.069379Z","shell.execute_reply.started":"2023-11-01T06:56:39.062438Z","shell.execute_reply":"2023-11-01T06:56:39.068553Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def load_dataset(filenames, labeled = True, ordered = False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n    \n    ignore_order = tf.data.Options()\n    if not ordered:\n        # disable order, increase speed\n        ignore_order.experimental_deterministic = False \n        \n    # automatically interleaves reads from multiple files\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n    # use data as soon as it streams in, rather than in its original order\n    dataset = dataset.with_options(ignore_order)\n    # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) \n    return dataset\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:56:54.726068Z","iopub.execute_input":"2023-11-01T06:56:54.726384Z","iopub.status.idle":"2023-11-01T06:56:54.731920Z","shell.execute_reply.started":"2023-11-01T06:56:54.726357Z","shell.execute_reply":"2023-11-01T06:56:54.731030Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# function for training and validation dataset\ndef setup_input1(image, label, data):\n    \n    # get anatom site general challenge vectors\n    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n    \n    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n    \n    tabular = tf.stack(tab_data + anatom)\n    \n    return {'inp1': image, 'inp2':  tabular}, label","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:57:05.709280Z","iopub.execute_input":"2023-11-01T06:57:05.709618Z","iopub.status.idle":"2023-11-01T06:57:05.715292Z","shell.execute_reply.started":"2023-11-01T06:57:05.709590Z","shell.execute_reply":"2023-11-01T06:57:05.714422Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# function for the test set\ndef setup_input2(image, image_name, data):\n    \n    # get anatom site general challenge vectors\n    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n    \n    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n    \n    tabular = tf.stack(tab_data + anatom)\n    \n    return {'inp1': image, 'inp2':  tabular}, image_name","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:57:20.227352Z","iopub.execute_input":"2023-11-01T06:57:20.227674Z","iopub.status.idle":"2023-11-01T06:57:20.233445Z","shell.execute_reply.started":"2023-11-01T06:57:20.227648Z","shell.execute_reply":"2023-11-01T06:57:20.232625Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# function for the validation (image name)\ndef setup_input3(image, image_name, target, data):\n    \n    # get anatom site general challenge vectors\n    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n    \n    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n    \n    tabular = tf.stack(tab_data + anatom)\n    \n    return {'inp1': image, 'inp2':  tabular}, image_name, target","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:57:34.069235Z","iopub.execute_input":"2023-11-01T06:57:34.069595Z","iopub.status.idle":"2023-11-01T06:57:34.075469Z","shell.execute_reply.started":"2023-11-01T06:57:34.069568Z","shell.execute_reply":"2023-11-01T06:57:34.074612Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def data_augment(data, label):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement \n    # in the next function (below), this happens essentially for free on TPU. \n    # Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    data['inp1'] = tf.image.random_flip_left_right(data['inp1'])\n    data['inp1'] = tf.image.random_flip_up_down(data['inp1'])\n    data['inp1'] = tf.image.random_hue(data['inp1'], 0.01)\n    data['inp1'] = tf.image.random_saturation(data['inp1'], 0.7, 1.3)\n    data['inp1'] = tf.image.random_contrast(data['inp1'], 0.8, 1.2)\n    data['inp1'] = tf.image.random_brightness(data['inp1'], 0.1)\n    #data['inp1'] = tf.image.random_crop(data['inp1'], 0.1)\n    #data['inp1'] = tf.image.central_crop(data['inp1'], 0.1)\n    \n    #data['inp1'] = data_augmentation(data['inp1'], label['inp1'])\n    return data, label","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:57:48.773033Z","iopub.execute_input":"2023-11-01T06:57:48.773670Z","iopub.status.idle":"2023-11-01T06:57:48.779470Z","shell.execute_reply.started":"2023-11-01T06:57:48.773639Z","shell.execute_reply":"2023-11-01T06:57:48.778595Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def data_augment_spatial(image, label):\n    p_spatial = tf.random.uniform([1], minval=0, maxval=1, dtype='float32')\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n\n    return image, label\n\ndef data_augment_rotate(image, label):\n    p_rotate = tf.random.uniform([1], minval=0, maxval=1, dtype='float32')\n    \n    if p_rotate > .66:\n        image = tf.image.rot90(image, k=3) # rotate 270º\n    elif p_rotate > .33:\n        image = tf.image.rot90(image, k=2) # rotate 180º\n    else:\n        image = tf.image.rot90(image, k=1) # rotate 90º\n\n    return image, label\n\ndef data_augment_crop(image, label):\n    p_crop = tf.random.uniform([1], minval=0, maxval=1, dtype='float32')\n    \n    if p_crop > .8:\n        image = tf.image.random_crop(image, size=[int(config['HEIGHT']*.7), int(config['WIDTH']*.7), config['CHANNELS']])\n    elif p_crop > .6:\n        image = tf.image.random_crop(image, size=[int(config['HEIGHT']*.8), int(config['WIDTH']*.8), config['CHANNELS']])\n    elif p_crop > .4:\n        image = tf.image.random_crop(image, size=[int(config['HEIGHT']*.9), int(config['WIDTH']*.9), config['CHANNELS']])\n    elif p_crop > .2:\n        image = tf.image.central_crop(image, central_fraction=.8)\n    else:\n        image = tf.image.central_crop(image, central_fraction=.7)\n    \n    image = tf.image.resize(image, size=[config['HEIGHT'], config['WIDTH']])\n\n    return image, label\n\ndef data_augment_rotation(image, label, max_angle=45.):\n    image = transform_rotation(image, config['HEIGHT'], max_angle)\n        \n    return image, label\n\ndef data_augment_shift(image, label):\n    image = transform_shift(image, config['HEIGHT'], 50., 50.)\n    return image, label\n\ndef data_augment_shear(image, label):\n    image = transform_shear(image, config['HEIGHT'], 25.)\n    return image, label\n\ndef data_augment_hue(image, label):\n    image = tf.image.random_hue(image, 0.02)\n    return image, label\n\ndef data_augment_saturation(image, label):\n    image = tf.image.random_saturation(image, 0.8, 1.2)\n    return image, label\n\ndef data_augment_contrast(image, label):\n    image = tf.image.random_contrast(image, 0.8, 1.2)\n    return image, label\n\ndef data_augment_brightness(image, label):\n    image = tf.image.random_brightness(image, 0.1)\n    return image, label\n\ndef data_augment_cutout(image, label):\n    p_cutout = tf.random.uniform([1], minval=0, maxval=1, dtype='float32')\n    \n    if p_cutout > .9: # 3 cut outs\n        image = random_cutout(image, config['HEIGHT'], config['WIDTH'], min_mask_size=(10, 10), max_mask_size=(80, 80), k=3)\n    elif p_cutout > .75: # 2 cut outs\n        image = random_cutout(image, config['HEIGHT'], config['WIDTH'], min_mask_size=(10, 10), max_mask_size=(80, 80), k=2)\n    else: # 1 cut out\n        image = random_cutout(image, config['HEIGHT'], config['WIDTH'], min_mask_size=(10, 10), max_mask_size=(80, 80), k=1)\n        \n    return image, label\n\ndef data_augmentation(image, label):\n    image, label = data_augment_spatial(image, label)\n    image, label = data_augment_rotate(image, label)\n    image, label = data_augment_crop(image, label)\n    image, label = data_augment_rotation(image, label)\n    image, label = data_augment_shift(image, label)\n    image, label = data_augment_shear(image, label)\n    image, label = data_augment_hue(image, label)\n    image, label = data_augment_saturation(image, label)\n    image, label = data_augment_contrast(image, label)\n    image, label = data_augment_brightness(image, label)\n    \n    return image, label","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:58:16.503798Z","iopub.execute_input":"2023-11-01T06:58:16.504124Z","iopub.status.idle":"2023-11-01T06:58:16.523473Z","shell.execute_reply.started":"2023-11-01T06:58:16.504098Z","shell.execute_reply":"2023-11-01T06:58:16.522684Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_training_dataset(filenames, labeled = True, ordered = False):\n    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n    dataset = dataset.map(setup_input1, num_parallel_calls = AUTO)\n    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n    dataset = dataset.map(transform, num_parallel_calls = AUTO)\n    # the training dataset must repeat for several epochs\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    # prefetch next batch while training (autotune prefetch buffer size)\n    \n    dataset = dataset.prefetch(AUTO)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:58:34.165214Z","iopub.execute_input":"2023-11-01T06:58:34.165539Z","iopub.status.idle":"2023-11-01T06:58:34.170966Z","shell.execute_reply.started":"2023-11-01T06:58:34.165515Z","shell.execute_reply":"2023-11-01T06:58:34.170184Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def get_validation_dataset(filenames, labeled = True, ordered = True):\n    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n    dataset = dataset.map(setup_input1, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    # using gpu, not enought memory to use cache\n    # dataset = dataset.cache()\n    # prefetch next batch while training (autotune prefetch buffer size)\n    dataset = dataset.prefetch(AUTO) \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:58:46.332767Z","iopub.execute_input":"2023-11-01T06:58:46.333149Z","iopub.status.idle":"2023-11-01T06:58:46.338267Z","shell.execute_reply.started":"2023-11-01T06:58:46.333118Z","shell.execute_reply":"2023-11-01T06:58:46.337415Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def get_test_dataset(filenames, labeled = False, ordered = True):\n    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n    dataset = dataset.map(setup_input2, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    # prefetch next batch while training (autotune prefetch buffer size)\n    dataset = dataset.prefetch(AUTO) \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:58:58.269933Z","iopub.execute_input":"2023-11-01T06:58:58.270277Z","iopub.status.idle":"2023-11-01T06:58:58.275541Z","shell.execute_reply.started":"2023-11-01T06:58:58.270251Z","shell.execute_reply":"2023-11-01T06:58:58.274614Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# function to count how many photos we have in\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\n# this function parse our images and also get the target variable\ndef read_tfrecord_full(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"image_name\": tf.io.FixedLenFeature([], tf.string), \n        \"target\": tf.io.FixedLenFeature([], tf.int64), \n        # meta features\n        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    image_name = example['image_name']\n    target = tf.cast(example['target'], tf.float32)\n    # meta features\n    data = {}\n    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n    data['sex'] = tf.cast(example['sex'], tf.int32)\n    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n    return image, image_name, target, data","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:59:16.910111Z","iopub.execute_input":"2023-11-01T06:59:16.910821Z","iopub.status.idle":"2023-11-01T06:59:16.919125Z","shell.execute_reply.started":"2023-11-01T06:59:16.910749Z","shell.execute_reply":"2023-11-01T06:59:16.918231Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def load_dataset_full(filenames):        \n    # automatically interleaves reads from multiple files\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n    # returns a dataset of (image_name, target)\n    dataset = dataset.map(read_tfrecord_full, num_parallel_calls = AUTO) \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-01T07:00:15.483890Z","iopub.execute_input":"2023-11-01T07:00:15.484218Z","iopub.status.idle":"2023-11-01T07:00:15.488519Z","shell.execute_reply.started":"2023-11-01T07:00:15.484193Z","shell.execute_reply":"2023-11-01T07:00:15.487646Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def get_data_full(filenames):\n    dataset = load_dataset_full(filenames)\n    dataset = dataset.map(setup_input3, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-01T07:00:27.061467Z","iopub.execute_input":"2023-11-01T07:00:27.061953Z","iopub.status.idle":"2023-11-01T07:00:27.066885Z","shell.execute_reply.started":"2023-11-01T07:00:27.061901Z","shell.execute_reply":"2023-11-01T07:00:27.066049Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"NUM_TRAINING_IMAGES = int(count_data_items(TRAINING_FILENAMES) * 0.8)\n# use validation data for training\nNUM_VALIDATION_IMAGES = int(count_data_items(TRAINING_FILENAMES) * 0.2)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n\nprint('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))","metadata":{"execution":{"iopub.status.busy":"2023-11-01T07:00:41.429212Z","iopub.execute_input":"2023-11-01T07:00:41.429533Z","iopub.status.idle":"2023-11-01T07:00:41.435558Z","shell.execute_reply.started":"2023-11-01T07:00:41.429508Z","shell.execute_reply":"2023-11-01T07:00:41.434653Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Dataset: 26153 training images, 6538 validation images, 10982 unlabeled test images\n","output_type":"stream"}]},{"cell_type":"code","source":"def binary_focal_loss(gamma=2., alpha=.25):\n    \"\"\"\n    Binary form of focal loss.\n      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n    References:\n        https://arxiv.org/pdf/1708.02002.pdf\n    Usage:\n     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n    \"\"\"\n    def binary_focal_loss_fixed(y_true, y_pred):\n        \"\"\"\n        :param y_true: A tensor of the same shape as `y_pred`\n        :param y_pred:  A tensor resulting from a sigmoid\n        :return: Output tensor.\n        \"\"\"\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n\n        epsilon = K.epsilon()\n        # clip to prevent NaN's and Inf's\n        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n\n    return binary_focal_loss_fixed","metadata":{"execution":{"iopub.status.busy":"2023-11-01T07:01:00.860775Z","iopub.execute_input":"2023-11-01T07:01:00.861112Z","iopub.status.idle":"2023-11-01T07:01:00.868251Z","shell.execute_reply.started":"2023-11-01T07:01:00.861080Z","shell.execute_reply":"2023-11-01T07:01:00.867472Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    \n    \n    with strategy.scope():\n        inp1 = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n        inp2 = tf.keras.layers.Input(shape = (9), name = 'inp2')\n        efnetb0 = efn.EfficientNetB6(weights = 'imagenet', include_top = False)\n        x = efnetb0(inp1)\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        x1 = tf.keras.layers.Dense(50)(inp2)\n        x1 = tf.keras.layers.BatchNormalization()(x1)\n        x1 = tf.keras.layers.Activation('relu')(x1)\n        concat = tf.keras.layers.concatenate([x, x1])\n        concat = tf.keras.layers.Dense(384, activation = 'relu')(concat)\n        concat = tf.keras.layers.BatchNormalization()(concat)\n        concat = tf.keras.layers.Dropout(0.2)(concat)\n        concat = tf.keras.layers.Dense(182, activation = 'relu')(concat)\n        concat = tf.keras.layers.BatchNormalization()(concat)\n        concat = tf.keras.layers.Dropout(0.2)(concat)\n        output = tf.keras.layers.Dense(1, activation = 'sigmoid')(concat)\n\n        model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output])\n\n        #opt = tf.keras.optimizers.Adam(learning_rate = LR)\n        # opt = tfa.optimizers.SWA(opt)\n        #opt = Lookahead(RAdam(lr=0.001))\n        #opt = RAdam(total_steps=5000, warmup_proportion=0.1,learning_rate = 1e-4)\n        \n        opt = tfa.optimizers.RectifiedAdam(lr=1e-3,total_steps=10000, warmup_proportion=0.1,min_lr=1e-5,)\n        opt = tfa.optimizers.Lookahead(opt, sync_period=6, slow_step_size=0.5)\n        \n        model.compile(\n            optimizer = opt,\n            loss = [binary_focal_loss(gamma = 2.0, alpha = 0.80)],\n            metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n        )\n        \n        return model","metadata":{"execution":{"iopub.status.busy":"2023-11-01T07:01:21.580440Z","iopub.execute_input":"2023-11-01T07:01:21.580846Z","iopub.status.idle":"2023-11-01T07:01:21.591741Z","shell.execute_reply.started":"2023-11-01T07:01:21.580814Z","shell.execute_reply":"2023-11-01T07:01:21.590928Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(\"Model Summary\")\nprint(get_model().summary())","metadata":{"execution":{"iopub.status.busy":"2023-11-01T07:01:33.803274Z","iopub.execute_input":"2023-11-01T07:01:33.803605Z","iopub.status.idle":"2023-11-01T07:02:29.087027Z","shell.execute_reply.started":"2023-11-01T07:01:33.803570Z","shell.execute_reply":"2023-11-01T07:02:29.085700Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Model Summary\nDownloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b6_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n165527152/165527152 [==============================] - 1s 0us/step\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n inp2 (InputLayer)              [(None, 9)]          0           []                               \n                                                                                                  \n inp1 (InputLayer)              [(None, 256, 256, 3  0           []                               \n                                )]                                                                \n                                                                                                  \n dense (Dense)                  (None, 50)           500         ['inp2[0][0]']                   \n                                                                                                  \n efficientnet-b6 (Functional)   (None, None, None,   40960136    ['inp1[0][0]']                   \n                                2304)                                                             \n                                                                                                  \n batch_normalization (BatchNorm  (None, 50)          200         ['dense[0][0]']                  \n alization)                                                                                       \n                                                                                                  \n global_average_pooling2d (Glob  (None, 2304)        0           ['efficientnet-b6[0][0]']        \n alAveragePooling2D)                                                                              \n                                                                                                  \n activation (Activation)        (None, 50)           0           ['batch_normalization[0][0]']    \n                                                                                                  \n concatenate (Concatenate)      (None, 2354)         0           ['global_average_pooling2d[0][0]'\n                                                                 , 'activation[0][0]']            \n                                                                                                  \n dense_1 (Dense)                (None, 384)          904320      ['concatenate[0][0]']            \n                                                                                                  \n batch_normalization_1 (BatchNo  (None, 384)         1536        ['dense_1[0][0]']                \n rmalization)                                                                                     \n                                                                                                  \n dropout (Dropout)              (None, 384)          0           ['batch_normalization_1[0][0]']  \n                                                                                                  \n dense_2 (Dense)                (None, 182)          70070       ['dropout[0][0]']                \n                                                                                                  \n batch_normalization_2 (BatchNo  (None, 182)         728         ['dense_2[0][0]']                \n rmalization)                                                                                     \n                                                                                                  \n dropout_1 (Dropout)            (None, 182)          0           ['batch_normalization_2[0][0]']  \n                                                                                                  \n dense_3 (Dense)                (None, 1)            183         ['dropout_1[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 41,937,673\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/tensorflow_addons/optimizers/rectified_adam.py:121: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super().__init__(name, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Trainable params: 41,712,009\nNon-trainable params: 225,664\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"file_path=\"/kaggle/working/effnet4_weights.best.hdf5\"\n\ncheckpoint = ModelCheckpoint(file_path, monitor='auc', verbose=1, save_best_only=True, mode='max')","metadata":{"execution":{"iopub.status.busy":"2023-11-01T07:02:43.599229Z","iopub.execute_input":"2023-11-01T07:02:43.599630Z","iopub.status.idle":"2023-11-01T07:02:43.604689Z","shell.execute_reply.started":"2023-11-01T07:02:43.599600Z","shell.execute_reply":"2023-11-01T07:02:43.603661Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def train_and_predict(SUB, folds = 2):\n    \n    models = []\n    oof_image_name = []\n    oof_target = []\n    oof_prediction = []\n    \n    # seed everything\n    seed_everything(SEED)\n\n    kfold = KFold(folds, shuffle = True, random_state = SEED)\n    #kfold = StratifiedKFold(folds, shuffle=True, random_state = SEED)\n    #for fold, (trn_ind, val_ind) in enumerate(kfold.split(TRAINING_FILENAMES,trn_ind)):\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(TRAINING_FILENAMES)):\n        print('\\n')\n        print('-'*50)\n        print(f'Training fold {fold + 1}')\n        train_dataset = get_training_dataset([TRAINING_FILENAMES[x] for x in trn_ind], labeled = True, ordered = False)\n        val_dataset = get_validation_dataset([TRAINING_FILENAMES[x] for x in val_ind], labeled = True, ordered = True)\n        K.clear_session()\n        model = get_model()\n       \n        # using early stopping using val loss\n        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_auc', mode = 'max', patience = 5, \n                                                      verbose = 1, min_delta = 0.0001, restore_best_weights = True)\n        # lr scheduler\n        cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_auc', factor = 0.4, patience = 2, verbose = 1, min_delta = 0.0001, mode = 'max')\n        history = model.fit(train_dataset, \n                            steps_per_epoch = STEPS_PER_EPOCH,\n                            epochs = EPOCHS,\n                            callbacks = [checkpoint, early_stopping, cb_lr_schedule],\n                            validation_data = val_dataset,\n                            verbose = 2)\n        models.append(model)\n        \n        # want to predict the validation set and save them for stacking\n        number_of_files = count_data_items([TRAINING_FILENAMES[x] for x in val_ind])\n        dataset = get_data_full([TRAINING_FILENAMES[x] for x in val_ind])\n        # get the image name\n        image_name = dataset.map(lambda image, image_name, target: image_name).unbatch()\n        image_name = next(iter(image_name.batch(number_of_files))).numpy().astype('U')\n        # get the real target\n        target = dataset.map(lambda image, image_name, target: target).unbatch()\n        target = next(iter(target.batch(number_of_files))).numpy()\n        # predict the validation set\n        image = dataset.map(lambda image, image_name, target: image)\n        probabilities = model.predict(image)\n        oof_image_name.extend(list(image_name))\n        oof_target.extend(list(target))\n        oof_prediction.extend(list(np.concatenate(probabilities)))\n    \n    print('\\n')\n    print('-'*50)\n    # save predictions\n    oof_df = pd.DataFrame({'image_name': oof_image_name, 'target': oof_target, 'predictions': oof_prediction})\n    oof_df.to_csv('Efficient_B6.csv', index = False)\n        \n    # since we are splitting the dataset and iterating separately on images and ids, order matters.\n    test_ds = get_test_dataset(TEST_FILENAMES, labeled = False, ordered = True)\n    test_images_ds = test_ds.map(lambda image, image_name: image)\n    \n    print('Computing predictions...')\n    probabilities = np.average([np.concatenate(models[i].predict(test_images_ds)) for i in range(folds)], axis = 0)\n    print('Generating submission.csv file...')\n    test_ids_ds = test_ds.map(lambda image, image_name: image_name).unbatch()\n    # all in one batch\n    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n    pred_df = pd.DataFrame({'image_name': test_ids, 'target': probabilities})\n    SUB.drop('target', inplace = True, axis = 1)\n    SUB = SUB.merge(pred_df, on = 'image_name')\n    SUB.to_csv('sub_Efficient_B6.csv', index = False)\n    \n    return oof_target, oof_prediction\n    \noof_target, oof_prediction = train_and_predict(SUB)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T07:02:46.058482Z","iopub.execute_input":"2023-11-01T07:02:46.059788Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\n\n--------------------------------------------------\nTraining fold 1\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"2023-11-01 07:06:02.984930: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 07:06:07.978545: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 07:12:35.885001: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 07:12:36.466374: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: auc improved from -inf to 0.50151, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 585s - loss: 0.7699 - binary_accuracy: 0.5232 - auc: 0.5015 - val_loss: 0.2997 - val_binary_accuracy: 0.6368 - val_auc: 0.5207 - lr: 0.0010 - 585s/epoch - 1s/step\nEpoch 2/25\n\nEpoch 2: auc improved from 0.50151 to 0.54676, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 96s - loss: 0.3457 - binary_accuracy: 0.7040 - auc: 0.5468 - val_loss: 0.1238 - val_binary_accuracy: 0.9621 - val_auc: 0.6579 - lr: 0.0010 - 96s/epoch - 234ms/step\nEpoch 3/25\n\nEpoch 3: auc improved from 0.54676 to 0.60583, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 97s - loss: 0.1701 - binary_accuracy: 0.8961 - auc: 0.6058 - val_loss: 0.1116 - val_binary_accuracy: 0.9813 - val_auc: 0.6685 - lr: 0.0010 - 97s/epoch - 237ms/step\nEpoch 4/25\n\nEpoch 4: auc improved from 0.60583 to 0.66421, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 97s - loss: 0.1386 - binary_accuracy: 0.9455 - auc: 0.6642 - val_loss: 0.0938 - val_binary_accuracy: 0.9824 - val_auc: 0.8022 - lr: 0.0010 - 97s/epoch - 237ms/step\nEpoch 5/25\n\nEpoch 5: auc improved from 0.66421 to 0.70655, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 93s - loss: 0.1256 - binary_accuracy: 0.9565 - auc: 0.7065 - val_loss: 0.1030 - val_binary_accuracy: 0.9811 - val_auc: 0.7868 - lr: 0.0010 - 93s/epoch - 228ms/step\nEpoch 6/25\n\nEpoch 6: auc improved from 0.70655 to 0.74377, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 97s - loss: 0.1156 - binary_accuracy: 0.9604 - auc: 0.7438 - val_loss: 0.0869 - val_binary_accuracy: 0.9821 - val_auc: 0.8564 - lr: 0.0010 - 97s/epoch - 237ms/step\nEpoch 7/25\n\nEpoch 7: auc improved from 0.74377 to 0.76300, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 92s - loss: 0.1100 - binary_accuracy: 0.9668 - auc: 0.7630 - val_loss: 0.0842 - val_binary_accuracy: 0.9801 - val_auc: 0.8564 - lr: 0.0010 - 92s/epoch - 227ms/step\nEpoch 8/25\n\nEpoch 8: auc improved from 0.76300 to 0.79741, saving model to /kaggle/working/effnet4_weights.best.hdf5\n\nEpoch 8: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n408/408 - 94s - loss: 0.1027 - binary_accuracy: 0.9668 - auc: 0.7974 - val_loss: 0.0912 - val_binary_accuracy: 0.9802 - val_auc: 0.8459 - lr: 0.0010 - 94s/epoch - 231ms/step\nEpoch 9/25\n\nEpoch 9: auc did not improve from 0.79741\n408/408 - 88s - loss: 0.1023 - binary_accuracy: 0.9696 - auc: 0.7915 - val_loss: 0.0842 - val_binary_accuracy: 0.9788 - val_auc: 0.8649 - lr: 4.0000e-04 - 88s/epoch - 215ms/step\nEpoch 10/25\n\nEpoch 10: auc improved from 0.79741 to 0.82241, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 95s - loss: 0.0965 - binary_accuracy: 0.9703 - auc: 0.8224 - val_loss: 0.0790 - val_binary_accuracy: 0.9758 - val_auc: 0.8756 - lr: 4.0000e-04 - 95s/epoch - 234ms/step\nEpoch 11/25\n\nEpoch 11: auc improved from 0.82241 to 0.82586, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 95s - loss: 0.0970 - binary_accuracy: 0.9684 - auc: 0.8259 - val_loss: 0.0766 - val_binary_accuracy: 0.9802 - val_auc: 0.8897 - lr: 4.0000e-04 - 95s/epoch - 232ms/step\nEpoch 12/25\n\nEpoch 12: auc improved from 0.82586 to 0.84157, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 93s - loss: 0.0897 - binary_accuracy: 0.9715 - auc: 0.8416 - val_loss: 0.0831 - val_binary_accuracy: 0.9782 - val_auc: 0.8753 - lr: 4.0000e-04 - 93s/epoch - 227ms/step\nEpoch 13/25\n\nEpoch 13: auc improved from 0.84157 to 0.85584, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 96s - loss: 0.0892 - binary_accuracy: 0.9693 - auc: 0.8558 - val_loss: 0.0750 - val_binary_accuracy: 0.9769 - val_auc: 0.8939 - lr: 4.0000e-04 - 96s/epoch - 236ms/step\nEpoch 14/25\n\nEpoch 14: auc did not improve from 0.85584\n408/408 - 87s - loss: 0.0876 - binary_accuracy: 0.9704 - auc: 0.8530 - val_loss: 0.0788 - val_binary_accuracy: 0.9703 - val_auc: 0.8861 - lr: 4.0000e-04 - 87s/epoch - 214ms/step\nEpoch 15/25\n\nEpoch 15: auc did not improve from 0.85584\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n408/408 - 86s - loss: 0.0901 - binary_accuracy: 0.9706 - auc: 0.8528 - val_loss: 0.0793 - val_binary_accuracy: 0.9785 - val_auc: 0.8865 - lr: 4.0000e-04 - 86s/epoch - 211ms/step\nEpoch 16/25\n\nEpoch 16: auc improved from 0.85584 to 0.86919, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 94s - loss: 0.0846 - binary_accuracy: 0.9706 - auc: 0.8692 - val_loss: 0.0786 - val_binary_accuracy: 0.9683 - val_auc: 0.8885 - lr: 1.6000e-04 - 94s/epoch - 229ms/step\nEpoch 17/25\n\nEpoch 17: auc improved from 0.86919 to 0.88291, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 96s - loss: 0.0797 - binary_accuracy: 0.9710 - auc: 0.8829 - val_loss: 0.0766 - val_binary_accuracy: 0.9621 - val_auc: 0.8972 - lr: 1.6000e-04 - 96s/epoch - 235ms/step\nEpoch 18/25\n\nEpoch 18: auc did not improve from 0.88291\n408/408 - 89s - loss: 0.0818 - binary_accuracy: 0.9725 - auc: 0.8784 - val_loss: 0.0753 - val_binary_accuracy: 0.9686 - val_auc: 0.8990 - lr: 1.6000e-04 - 89s/epoch - 217ms/step\nEpoch 19/25\n\nEpoch 19: auc improved from 0.88291 to 0.88421, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 93s - loss: 0.0794 - binary_accuracy: 0.9719 - auc: 0.8842 - val_loss: 0.0792 - val_binary_accuracy: 0.9687 - val_auc: 0.8907 - lr: 1.6000e-04 - 93s/epoch - 228ms/step\nEpoch 20/25\n\nEpoch 20: auc improved from 0.88421 to 0.88859, saving model to /kaggle/working/effnet4_weights.best.hdf5\n\nEpoch 20: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n408/408 - 96s - loss: 0.0792 - binary_accuracy: 0.9704 - auc: 0.8886 - val_loss: 0.0797 - val_binary_accuracy: 0.9681 - val_auc: 0.8949 - lr: 1.6000e-04 - 96s/epoch - 234ms/step\nEpoch 21/25\n\nEpoch 21: auc improved from 0.88859 to 0.90347, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 94s - loss: 0.0757 - binary_accuracy: 0.9696 - auc: 0.9035 - val_loss: 0.0784 - val_binary_accuracy: 0.9667 - val_auc: 0.8960 - lr: 6.4000e-05 - 94s/epoch - 231ms/step\nEpoch 22/25\n\nEpoch 22: auc did not improve from 0.90347\n408/408 - 88s - loss: 0.0771 - binary_accuracy: 0.9718 - auc: 0.8984 - val_loss: 0.0771 - val_binary_accuracy: 0.9663 - val_auc: 0.9009 - lr: 6.4000e-05 - 88s/epoch - 216ms/step\nEpoch 23/25\n\nEpoch 23: auc improved from 0.90347 to 0.90435, saving model to /kaggle/working/effnet4_weights.best.hdf5\n408/408 - 93s - loss: 0.0732 - binary_accuracy: 0.9712 - auc: 0.9044 - val_loss: 0.0779 - val_binary_accuracy: 0.9684 - val_auc: 0.8961 - lr: 6.4000e-05 - 93s/epoch - 228ms/step\nEpoch 24/25\n\nEpoch 24: auc improved from 0.90435 to 0.90444, saving model to /kaggle/working/effnet4_weights.best.hdf5\n\nEpoch 24: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.\n408/408 - 94s - loss: 0.0742 - binary_accuracy: 0.9727 - auc: 0.9044 - val_loss: 0.0783 - val_binary_accuracy: 0.9678 - val_auc: 0.8981 - lr: 6.4000e-05 - 94s/epoch - 231ms/step\nEpoch 25/25\n\nEpoch 25: auc did not improve from 0.90444\n408/408 - 86s - loss: 0.0802 - binary_accuracy: 0.9717 - auc: 0.8848 - val_loss: 0.0774 - val_binary_accuracy: 0.9690 - val_auc: 0.8995 - lr: 2.5600e-05 - 86s/epoch - 211ms/step\n","output_type":"stream"},{"name":"stderr","text":"2023-11-01 07:50:26.772978: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-11-01 07:50:27.243372: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"102/102 [==============================] - 26s 54ms/step\n\n\n--------------------------------------------------\nTraining fold 2\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"2023-11-01 07:54:05.023166: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 07:54:09.884089: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 07:55:48.935215: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 07:55:49.562038: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: auc did not improve from 0.90444\n408/408 - 276s - loss: 0.7846 - binary_accuracy: 0.5209 - auc: 0.5535 - val_loss: 0.3781 - val_binary_accuracy: 0.5072 - val_auc: 0.5323 - lr: 0.0010 - 276s/epoch - 677ms/step\nEpoch 2/25\n\nEpoch 2: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.3666 - binary_accuracy: 0.6943 - auc: 0.5379 - val_loss: 0.1352 - val_binary_accuracy: 0.9399 - val_auc: 0.6453 - lr: 0.0010 - 91s/epoch - 222ms/step\nEpoch 3/25\n\nEpoch 3: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.1806 - binary_accuracy: 0.8899 - auc: 0.5898 - val_loss: 0.1112 - val_binary_accuracy: 0.9738 - val_auc: 0.7168 - lr: 0.0010 - 90s/epoch - 221ms/step\nEpoch 4/25\n\nEpoch 4: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.1454 - binary_accuracy: 0.9378 - auc: 0.6520 - val_loss: 0.1030 - val_binary_accuracy: 0.9651 - val_auc: 0.7795 - lr: 0.0010 - 91s/epoch - 222ms/step\nEpoch 5/25\n\nEpoch 5: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.1272 - binary_accuracy: 0.9548 - auc: 0.6885 - val_loss: 0.1023 - val_binary_accuracy: 0.9810 - val_auc: 0.7944 - lr: 0.0010 - 90s/epoch - 221ms/step\nEpoch 6/25\n\nEpoch 6: auc did not improve from 0.90444\n408/408 - 89s - loss: 0.1129 - binary_accuracy: 0.9605 - auc: 0.7596 - val_loss: 0.1294 - val_binary_accuracy: 0.9709 - val_auc: 0.7513 - lr: 0.0010 - 89s/epoch - 218ms/step\nEpoch 7/25\n\nEpoch 7: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.1128 - binary_accuracy: 0.9668 - auc: 0.7487 - val_loss: 0.0989 - val_binary_accuracy: 0.9803 - val_auc: 0.8154 - lr: 0.0010 - 91s/epoch - 223ms/step\nEpoch 8/25\n\nEpoch 8: auc did not improve from 0.90444\n408/408 - 89s - loss: 0.1094 - binary_accuracy: 0.9691 - auc: 0.7534 - val_loss: 0.1201 - val_binary_accuracy: 0.9821 - val_auc: 0.7203 - lr: 0.0010 - 89s/epoch - 218ms/step\nEpoch 9/25\n\nEpoch 9: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.1058 - binary_accuracy: 0.9683 - auc: 0.7745 - val_loss: 0.0928 - val_binary_accuracy: 0.9569 - val_auc: 0.8604 - lr: 0.0010 - 91s/epoch - 222ms/step\nEpoch 10/25\n\nEpoch 10: auc did not improve from 0.90444\n408/408 - 89s - loss: 0.1070 - binary_accuracy: 0.9695 - auc: 0.7742 - val_loss: 0.1009 - val_binary_accuracy: 0.9537 - val_auc: 0.8252 - lr: 0.0010 - 89s/epoch - 218ms/step\nEpoch 11/25\n\nEpoch 11: auc did not improve from 0.90444\n\nEpoch 11: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n408/408 - 88s - loss: 0.1063 - binary_accuracy: 0.9724 - auc: 0.7577 - val_loss: 0.0904 - val_binary_accuracy: 0.9742 - val_auc: 0.8507 - lr: 0.0010 - 88s/epoch - 217ms/step\nEpoch 12/25\n\nEpoch 12: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.0969 - binary_accuracy: 0.9747 - auc: 0.8010 - val_loss: 0.0899 - val_binary_accuracy: 0.9725 - val_auc: 0.8607 - lr: 4.0000e-04 - 91s/epoch - 223ms/step\nEpoch 13/25\n\nEpoch 13: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.0959 - binary_accuracy: 0.9732 - auc: 0.8144 - val_loss: 0.0910 - val_binary_accuracy: 0.9648 - val_auc: 0.8632 - lr: 4.0000e-04 - 90s/epoch - 222ms/step\nEpoch 14/25\n\nEpoch 14: auc did not improve from 0.90444\n408/408 - 88s - loss: 0.0920 - binary_accuracy: 0.9732 - auc: 0.8258 - val_loss: 0.0907 - val_binary_accuracy: 0.9656 - val_auc: 0.8603 - lr: 4.0000e-04 - 88s/epoch - 215ms/step\nEpoch 15/25\n\nEpoch 15: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.0925 - binary_accuracy: 0.9738 - auc: 0.8245 - val_loss: 0.0885 - val_binary_accuracy: 0.9711 - val_auc: 0.8667 - lr: 4.0000e-04 - 90s/epoch - 222ms/step\nEpoch 16/25\n\nEpoch 16: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.0916 - binary_accuracy: 0.9717 - auc: 0.8405 - val_loss: 0.0889 - val_binary_accuracy: 0.9668 - val_auc: 0.8741 - lr: 4.0000e-04 - 90s/epoch - 220ms/step\nEpoch 17/25\n\nEpoch 17: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.0913 - binary_accuracy: 0.9728 - auc: 0.8309 - val_loss: 0.0866 - val_binary_accuracy: 0.9559 - val_auc: 0.8797 - lr: 4.0000e-04 - 91s/epoch - 223ms/step\nEpoch 18/25\n\nEpoch 18: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.0920 - binary_accuracy: 0.9734 - auc: 0.8316 - val_loss: 0.0823 - val_binary_accuracy: 0.9712 - val_auc: 0.8817 - lr: 4.0000e-04 - 91s/epoch - 223ms/step\nEpoch 19/25\n\nEpoch 19: auc did not improve from 0.90444\n408/408 - 92s - loss: 0.0892 - binary_accuracy: 0.9724 - auc: 0.8450 - val_loss: 0.0825 - val_binary_accuracy: 0.9671 - val_auc: 0.8830 - lr: 4.0000e-04 - 92s/epoch - 225ms/step\nEpoch 20/25\n\nEpoch 20: auc did not improve from 0.90444\n408/408 - 88s - loss: 0.0885 - binary_accuracy: 0.9734 - auc: 0.8428 - val_loss: 0.0846 - val_binary_accuracy: 0.9660 - val_auc: 0.8785 - lr: 4.0000e-04 - 88s/epoch - 216ms/step\nEpoch 21/25\n\nEpoch 21: auc did not improve from 0.90444\n\nEpoch 21: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n408/408 - 89s - loss: 0.0844 - binary_accuracy: 0.9740 - auc: 0.8629 - val_loss: 0.0859 - val_binary_accuracy: 0.9631 - val_auc: 0.8797 - lr: 4.0000e-04 - 89s/epoch - 217ms/step\nEpoch 22/25\n\nEpoch 22: auc did not improve from 0.90444\n408/408 - 88s - loss: 0.0855 - binary_accuracy: 0.9725 - auc: 0.8614 - val_loss: 0.0849 - val_binary_accuracy: 0.9677 - val_auc: 0.8806 - lr: 1.6000e-04 - 88s/epoch - 216ms/step\nEpoch 23/25\n\nEpoch 23: auc did not improve from 0.90444\n\nEpoch 23: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n408/408 - 89s - loss: 0.0838 - binary_accuracy: 0.9728 - auc: 0.8654 - val_loss: 0.0867 - val_binary_accuracy: 0.9628 - val_auc: 0.8815 - lr: 1.6000e-04 - 89s/epoch - 218ms/step\nEpoch 24/25\n\nEpoch 24: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.0854 - binary_accuracy: 0.9736 - auc: 0.8571 - val_loss: 0.0845 - val_binary_accuracy: 0.9664 - val_auc: 0.8861 - lr: 6.4000e-05 - 90s/epoch - 221ms/step\nEpoch 25/25\n\nEpoch 25: auc did not improve from 0.90444\n408/408 - 88s - loss: 0.0813 - binary_accuracy: 0.9739 - auc: 0.8744 - val_loss: 0.0862 - val_binary_accuracy: 0.9662 - val_auc: 0.8837 - lr: 6.4000e-05 - 88s/epoch - 217ms/step\n","output_type":"stream"},{"name":"stderr","text":"2023-11-01 08:32:05.043173: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-11-01 08:32:05.491384: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"103/103 [==============================] - 26s 193ms/step\n\n\n--------------------------------------------------\nTraining fold 3\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"2023-11-01 08:35:46.848687: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 08:35:51.732904: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 08:37:31.542349: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 08:37:32.135809: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: auc did not improve from 0.90444\n408/408 - 278s - loss: 0.7260 - binary_accuracy: 0.5218 - auc: 0.5546 - val_loss: 0.3204 - val_binary_accuracy: 0.6112 - val_auc: 0.6473 - lr: 0.0010 - 278s/epoch - 682ms/step\nEpoch 2/25\n\nEpoch 2: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.3511 - binary_accuracy: 0.6959 - auc: 0.5355 - val_loss: 0.1384 - val_binary_accuracy: 0.9343 - val_auc: 0.7509 - lr: 0.0010 - 91s/epoch - 224ms/step\nEpoch 3/25\n\nEpoch 3: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.1753 - binary_accuracy: 0.8938 - auc: 0.5893 - val_loss: 0.1009 - val_binary_accuracy: 0.9698 - val_auc: 0.7717 - lr: 0.0010 - 91s/epoch - 223ms/step\nEpoch 4/25\n\nEpoch 4: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.1405 - binary_accuracy: 0.9457 - auc: 0.6409 - val_loss: 0.0899 - val_binary_accuracy: 0.9753 - val_auc: 0.8362 - lr: 0.0010 - 90s/epoch - 221ms/step\nEpoch 5/25\n\nEpoch 5: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.1259 - binary_accuracy: 0.9568 - auc: 0.6895 - val_loss: 0.0867 - val_binary_accuracy: 0.9752 - val_auc: 0.8490 - lr: 0.0010 - 91s/epoch - 223ms/step\nEpoch 6/25\n\nEpoch 6: auc did not improve from 0.90444\n408/408 - 88s - loss: 0.1174 - binary_accuracy: 0.9639 - auc: 0.7210 - val_loss: 0.1160 - val_binary_accuracy: 0.9810 - val_auc: 0.8484 - lr: 0.0010 - 88s/epoch - 216ms/step\nEpoch 7/25\n\nEpoch 7: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.1119 - binary_accuracy: 0.9695 - auc: 0.7261 - val_loss: 0.0909 - val_binary_accuracy: 0.9821 - val_auc: 0.8538 - lr: 0.0010 - 91s/epoch - 223ms/step\nEpoch 8/25\n\nEpoch 8: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.1057 - binary_accuracy: 0.9692 - auc: 0.7717 - val_loss: 0.0885 - val_binary_accuracy: 0.9795 - val_auc: 0.8765 - lr: 0.0010 - 91s/epoch - 222ms/step\nEpoch 9/25\n\nEpoch 9: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.1066 - binary_accuracy: 0.9716 - auc: 0.7693 - val_loss: 0.0818 - val_binary_accuracy: 0.9819 - val_auc: 0.8795 - lr: 0.0010 - 90s/epoch - 222ms/step\nEpoch 10/25\n\nEpoch 10: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.0991 - binary_accuracy: 0.9750 - auc: 0.7918 - val_loss: 0.0789 - val_binary_accuracy: 0.9816 - val_auc: 0.8877 - lr: 0.0010 - 90s/epoch - 222ms/step\nEpoch 11/25\n\nEpoch 11: auc did not improve from 0.90444\n408/408 - 88s - loss: 0.1043 - binary_accuracy: 0.9742 - auc: 0.7672 - val_loss: 0.0808 - val_binary_accuracy: 0.9821 - val_auc: 0.8835 - lr: 0.0010 - 88s/epoch - 217ms/step\nEpoch 12/25\n\nEpoch 12: auc did not improve from 0.90444\n\nEpoch 12: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n408/408 - 88s - loss: 0.1037 - binary_accuracy: 0.9732 - auc: 0.7840 - val_loss: 0.0836 - val_binary_accuracy: 0.9798 - val_auc: 0.8766 - lr: 0.0010 - 88s/epoch - 217ms/step\nEpoch 13/25\n\nEpoch 13: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.0964 - binary_accuracy: 0.9750 - auc: 0.8065 - val_loss: 0.0766 - val_binary_accuracy: 0.9793 - val_auc: 0.8980 - lr: 4.0000e-04 - 90s/epoch - 220ms/step\nEpoch 14/25\n\nEpoch 14: auc did not improve from 0.90444\n408/408 - 87s - loss: 0.0968 - binary_accuracy: 0.9744 - auc: 0.8129 - val_loss: 0.0882 - val_binary_accuracy: 0.9778 - val_auc: 0.8547 - lr: 4.0000e-04 - 87s/epoch - 214ms/step\nEpoch 15/25\n\nEpoch 15: auc did not improve from 0.90444\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n408/408 - 88s - loss: 0.0951 - binary_accuracy: 0.9732 - auc: 0.8175 - val_loss: 0.0779 - val_binary_accuracy: 0.9730 - val_auc: 0.8897 - lr: 4.0000e-04 - 88s/epoch - 216ms/step\nEpoch 16/25\n\nEpoch 16: auc did not improve from 0.90444\n408/408 - 88s - loss: 0.0908 - binary_accuracy: 0.9744 - auc: 0.8336 - val_loss: 0.0765 - val_binary_accuracy: 0.9701 - val_auc: 0.8963 - lr: 1.6000e-04 - 88s/epoch - 216ms/step\nEpoch 17/25\n\nEpoch 17: auc did not improve from 0.90444\n\nEpoch 17: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n408/408 - 87s - loss: 0.0924 - binary_accuracy: 0.9748 - auc: 0.8292 - val_loss: 0.0788 - val_binary_accuracy: 0.9729 - val_auc: 0.8871 - lr: 1.6000e-04 - 87s/epoch - 214ms/step\nEpoch 18/25\n\nEpoch 18: auc did not improve from 0.90444\nRestoring model weights from the end of the best epoch: 13.\n408/408 - 101s - loss: 0.0905 - binary_accuracy: 0.9765 - auc: 0.8368 - val_loss: 0.0774 - val_binary_accuracy: 0.9687 - val_auc: 0.8965 - lr: 6.4000e-05 - 101s/epoch - 248ms/step\nEpoch 18: early stopping\n","output_type":"stream"},{"name":"stderr","text":"2023-11-01 09:03:25.339196: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-11-01 09:03:25.791002: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"102/102 [==============================] - 12s 56ms/step\n\n\n--------------------------------------------------\nTraining fold 4\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"2023-11-01 09:06:49.326122: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 09:06:54.173169: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 09:08:33.630239: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 09:08:34.180661: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: auc did not improve from 0.90444\n408/408 - 290s - loss: 0.7636 - binary_accuracy: 0.5255 - auc: 0.5501 - val_loss: 0.2972 - val_binary_accuracy: 0.6580 - val_auc: 0.5628 - lr: 0.0010 - 290s/epoch - 710ms/step\nEpoch 2/25\n\nEpoch 2: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.3507 - binary_accuracy: 0.6986 - auc: 0.5849 - val_loss: 0.1711 - val_binary_accuracy: 0.8694 - val_auc: 0.6335 - lr: 0.0010 - 91s/epoch - 223ms/step\nEpoch 3/25\n\nEpoch 3: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.1809 - binary_accuracy: 0.8926 - auc: 0.5939 - val_loss: 0.1030 - val_binary_accuracy: 0.9798 - val_auc: 0.7589 - lr: 0.0010 - 91s/epoch - 224ms/step\nEpoch 4/25\n\nEpoch 4: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.1467 - binary_accuracy: 0.9395 - auc: 0.6353 - val_loss: 0.0947 - val_binary_accuracy: 0.9792 - val_auc: 0.8056 - lr: 0.0010 - 91s/epoch - 223ms/step\nEpoch 5/25\n\nEpoch 5: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.1296 - binary_accuracy: 0.9542 - auc: 0.6873 - val_loss: 0.0894 - val_binary_accuracy: 0.9743 - val_auc: 0.8443 - lr: 0.0010 - 91s/epoch - 223ms/step\nEpoch 6/25\n\nEpoch 6: auc did not improve from 0.90444\n408/408 - 89s - loss: 0.1203 - binary_accuracy: 0.9587 - auc: 0.7335 - val_loss: 0.0988 - val_binary_accuracy: 0.9755 - val_auc: 0.8208 - lr: 0.0010 - 89s/epoch - 219ms/step\nEpoch 7/25\n\nEpoch 7: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.1119 - binary_accuracy: 0.9646 - auc: 0.7503 - val_loss: 0.0809 - val_binary_accuracy: 0.9804 - val_auc: 0.8769 - lr: 0.0010 - 91s/epoch - 223ms/step\nEpoch 8/25\n\nEpoch 8: auc did not improve from 0.90444\n408/408 - 89s - loss: 0.1065 - binary_accuracy: 0.9674 - auc: 0.7756 - val_loss: 0.0833 - val_binary_accuracy: 0.9761 - val_auc: 0.8703 - lr: 0.0010 - 89s/epoch - 218ms/step\nEpoch 9/25\n\nEpoch 9: auc did not improve from 0.90444\n\nEpoch 9: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n408/408 - 89s - loss: 0.1004 - binary_accuracy: 0.9697 - auc: 0.7970 - val_loss: 0.0898 - val_binary_accuracy: 0.9733 - val_auc: 0.8476 - lr: 0.0010 - 89s/epoch - 218ms/step\nEpoch 10/25\n\nEpoch 10: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.0963 - binary_accuracy: 0.9690 - auc: 0.8229 - val_loss: 0.0808 - val_binary_accuracy: 0.9781 - val_auc: 0.8785 - lr: 4.0000e-04 - 91s/epoch - 223ms/step\nEpoch 11/25\n\nEpoch 11: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.0991 - binary_accuracy: 0.9703 - auc: 0.8075 - val_loss: 0.0759 - val_binary_accuracy: 0.9767 - val_auc: 0.8972 - lr: 4.0000e-04 - 91s/epoch - 222ms/step\nEpoch 12/25\n\nEpoch 12: auc did not improve from 0.90444\n408/408 - 88s - loss: 0.0942 - binary_accuracy: 0.9724 - auc: 0.8224 - val_loss: 0.0759 - val_binary_accuracy: 0.9767 - val_auc: 0.8973 - lr: 4.0000e-04 - 88s/epoch - 216ms/step\nEpoch 13/25\n\nEpoch 13: auc did not improve from 0.90444\n\nEpoch 13: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n408/408 - 89s - loss: 0.0919 - binary_accuracy: 0.9717 - auc: 0.8349 - val_loss: 0.0771 - val_binary_accuracy: 0.9813 - val_auc: 0.8956 - lr: 4.0000e-04 - 89s/epoch - 218ms/step\nEpoch 14/25\n\nEpoch 14: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.0874 - binary_accuracy: 0.9731 - auc: 0.8535 - val_loss: 0.0764 - val_binary_accuracy: 0.9684 - val_auc: 0.8981 - lr: 1.6000e-04 - 90s/epoch - 221ms/step\nEpoch 15/25\n\nEpoch 15: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.0882 - binary_accuracy: 0.9722 - auc: 0.8541 - val_loss: 0.0751 - val_binary_accuracy: 0.9769 - val_auc: 0.8994 - lr: 1.6000e-04 - 91s/epoch - 223ms/step\nEpoch 16/25\n\nEpoch 16: auc did not improve from 0.90444\n408/408 - 92s - loss: 0.0862 - binary_accuracy: 0.9712 - auc: 0.8593 - val_loss: 0.0750 - val_binary_accuracy: 0.9749 - val_auc: 0.8996 - lr: 1.6000e-04 - 92s/epoch - 226ms/step\nEpoch 17/25\n\nEpoch 17: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.0839 - binary_accuracy: 0.9729 - auc: 0.8713 - val_loss: 0.0730 - val_binary_accuracy: 0.9741 - val_auc: 0.9048 - lr: 1.6000e-04 - 91s/epoch - 223ms/step\nEpoch 18/25\n\nEpoch 18: auc did not improve from 0.90444\n408/408 - 88s - loss: 0.0823 - binary_accuracy: 0.9723 - auc: 0.8719 - val_loss: 0.0769 - val_binary_accuracy: 0.9721 - val_auc: 0.8977 - lr: 1.6000e-04 - 88s/epoch - 216ms/step\nEpoch 19/25\n\nEpoch 19: auc did not improve from 0.90444\n\nEpoch 19: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n408/408 - 88s - loss: 0.0837 - binary_accuracy: 0.9719 - auc: 0.8710 - val_loss: 0.0743 - val_binary_accuracy: 0.9744 - val_auc: 0.9027 - lr: 1.6000e-04 - 88s/epoch - 216ms/step\nEpoch 20/25\n\nEpoch 20: auc did not improve from 0.90444\n408/408 - 89s - loss: 0.0801 - binary_accuracy: 0.9732 - auc: 0.8799 - val_loss: 0.0764 - val_binary_accuracy: 0.9680 - val_auc: 0.9033 - lr: 6.4000e-05 - 89s/epoch - 217ms/step\nEpoch 21/25\n\nEpoch 21: auc did not improve from 0.90444\n\nEpoch 21: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.\n408/408 - 89s - loss: 0.0871 - binary_accuracy: 0.9707 - auc: 0.8638 - val_loss: 0.0767 - val_binary_accuracy: 0.9674 - val_auc: 0.9039 - lr: 6.4000e-05 - 89s/epoch - 217ms/step\nEpoch 22/25\n\nEpoch 22: auc did not improve from 0.90444\nRestoring model weights from the end of the best epoch: 17.\n408/408 - 102s - loss: 0.0786 - binary_accuracy: 0.9720 - auc: 0.8879 - val_loss: 0.0767 - val_binary_accuracy: 0.9681 - val_auc: 0.9036 - lr: 2.5600e-05 - 102s/epoch - 249ms/step\nEpoch 22: early stopping\n","output_type":"stream"},{"name":"stderr","text":"2023-11-01 09:40:50.040544: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-11-01 09:40:50.511519: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"103/103 [==============================] - 12s 56ms/step\n\n\n--------------------------------------------------\nTraining fold 5\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"2023-11-01 09:44:16.386172: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 09:44:21.247960: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 09:46:02.039399: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-11-01 09:46:02.679986: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: auc did not improve from 0.90444\n408/408 - 278s - loss: 0.7573 - binary_accuracy: 0.5209 - auc: 0.5355 - val_loss: 0.3424 - val_binary_accuracy: 0.5479 - val_auc: 0.5489 - lr: 0.0010 - 278s/epoch - 681ms/step\nEpoch 2/25\n\nEpoch 2: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.3475 - binary_accuracy: 0.7002 - auc: 0.5685 - val_loss: 0.1329 - val_binary_accuracy: 0.9442 - val_auc: 0.6710 - lr: 0.0010 - 90s/epoch - 221ms/step\nEpoch 3/25\n\nEpoch 3: auc did not improve from 0.90444\n408/408 - 91s - loss: 0.1762 - binary_accuracy: 0.8988 - auc: 0.5857 - val_loss: 0.1003 - val_binary_accuracy: 0.9821 - val_auc: 0.7787 - lr: 0.0010 - 91s/epoch - 223ms/step\nEpoch 4/25\n\nEpoch 4: auc did not improve from 0.90444\n408/408 - 88s - loss: 0.1453 - binary_accuracy: 0.9460 - auc: 0.6429 - val_loss: 0.0975 - val_binary_accuracy: 0.9783 - val_auc: 0.7724 - lr: 0.0010 - 88s/epoch - 216ms/step\nEpoch 5/25\n\nEpoch 5: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.1345 - binary_accuracy: 0.9571 - auc: 0.6641 - val_loss: 0.1001 - val_binary_accuracy: 0.9796 - val_auc: 0.7981 - lr: 0.0010 - 90s/epoch - 220ms/step\nEpoch 6/25\n\nEpoch 6: auc did not improve from 0.90444\n408/408 - 92s - loss: 0.1218 - binary_accuracy: 0.9601 - auc: 0.7047 - val_loss: 0.0978 - val_binary_accuracy: 0.9825 - val_auc: 0.8394 - lr: 0.0010 - 92s/epoch - 225ms/step\nEpoch 7/25\n\nEpoch 7: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.1179 - binary_accuracy: 0.9637 - auc: 0.7313 - val_loss: 0.0871 - val_binary_accuracy: 0.9781 - val_auc: 0.8657 - lr: 0.0010 - 90s/epoch - 220ms/step\nEpoch 8/25\n\nEpoch 8: auc did not improve from 0.90444\n408/408 - 89s - loss: 0.1056 - binary_accuracy: 0.9698 - auc: 0.7649 - val_loss: 0.0849 - val_binary_accuracy: 0.9805 - val_auc: 0.8643 - lr: 0.0010 - 89s/epoch - 217ms/step\nEpoch 9/25\n\nEpoch 9: auc did not improve from 0.90444\n408/408 - 89s - loss: 0.1046 - binary_accuracy: 0.9705 - auc: 0.7777 - val_loss: 0.0866 - val_binary_accuracy: 0.9822 - val_auc: 0.8674 - lr: 0.0010 - 89s/epoch - 219ms/step\nEpoch 10/25\n\nEpoch 10: auc did not improve from 0.90444\n408/408 - 90s - loss: 0.0989 - binary_accuracy: 0.9723 - auc: 0.8003 - val_loss: 0.0772 - val_binary_accuracy: 0.9821 - val_auc: 0.8903 - lr: 0.0010 - 90s/epoch - 220ms/step\nEpoch 11/25\n\nEpoch 11: auc did not improve from 0.90444\n408/408 - 89s - loss: 0.0982 - binary_accuracy: 0.9716 - auc: 0.8029 - val_loss: 0.0857 - val_binary_accuracy: 0.9799 - val_auc: 0.8717 - lr: 0.0010 - 89s/epoch - 217ms/step\nEpoch 12/25\n","output_type":"stream"}]},{"cell_type":"code","source":"# calculate our out of folds roc auc score\nroc_auc = metrics.roc_auc_score(oof_target, oof_prediction)\nprint('Our out of folds roc auc score is: ', roc_auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['auc'])\nplt.title('model auc')\nplt.ylabel('auc')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.savefig('auc.jpg')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.savefig('loss.jpg')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}